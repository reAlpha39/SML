{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfrJMDUyaKYo"
      },
      "source": [
        "# Sequence to sequence learning for performing number addition\n",
        "\n",
        "**Author:** [Smerity](https://twitter.com/Smerity) and others<br>\n",
        "**Date created:** 2015/08/17<br>\n",
        "**Last modified:** 2020/04/17<br>\n",
        "**Description:** A model that learns to add strings of numbers, e.g. \"535+61\" -> \"596\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvYNhGl8aKYt"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we train a model to learn to add two numbers, provided as strings.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- Input: \"535+61\"\n",
        "- Output: \"596\"\n",
        "\n",
        "Input may optionally be reversed, which was shown to increase performance in many tasks\n",
        " in: [Learning to Execute](http://arxiv.org/abs/1410.4615) and\n",
        "[Sequence to Sequence Learning with Neural Networks](\n",
        "\n",
        " http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
        "\n",
        "Theoretically, sequence order inversion introduces shorter term dependencies between\n",
        " source and target for this problem.\n",
        "\n",
        "**Results:**\n",
        "\n",
        "For two digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
        "\n",
        "Three digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
        "\n",
        "Four digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
        "\n",
        "Five digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3audn6XqaKYu"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj6iu6qiaKYv"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = True\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTKEoKgiaKYw"
      },
      "source": [
        "## Generate the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWsz-tvJaKYx",
        "outputId": "d4cf487b-55f2-43dd-b592-97860798b877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating data...\n",
            "Total questions: 50000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class CharacterTable:\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return \"\".join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = \"0123456789+ \"\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print(\"Generating data...\")\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(\n",
        "        \"\".join(\n",
        "            np.random.choice(list(\"0123456789\"))\n",
        "            for i in range(np.random.randint(1, DIGITS + 1))\n",
        "        )\n",
        "    )\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = \"{}+{}\".format(a, b)\n",
        "    query = q + \" \" * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print(\"Total questions:\", len(questions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg_xjiWaaKYy"
      },
      "source": [
        "## Vectorize the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf4uvXw7aKYz",
        "outputId": "549ee4b9-0530-4969-aacb-1832fa8a8c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorization...\n",
            "Training Data:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "Validation Data:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n"
          ]
        }
      ],
      "source": [
        "print(\"Vectorization...\")\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "# digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"Validation Data:\")\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCteC3QKaKY0"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxZbQmD9aKY1",
        "outputId": "523cc049-1bc8-4c7a-9a63-4acea03ab3cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 128)               72192     \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 4, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 4, 128)            131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4, 12)             1548      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 205,324\n",
            "Trainable params: 205,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "print(\"Build model...\")\n",
        "num_layers = 1  # Try to add more LSTM layers!\n",
        "\n",
        "model = keras.Sequential()\n",
        "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
        "# Note: In a situation where your input sequences have a variable length,\n",
        "# use input_shape=(None, num_feature).\n",
        "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
        "# As the decoder RNN's input, repeatedly provide with the last output of\n",
        "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "# The decoder RNN could be multiple layers stacked or a single layer.\n",
        "for _ in range(num_layers):\n",
        "    # By setting return_sequences to True, return not only the last output but\n",
        "    # all the outputs so far in the form of (num_samples, timesteps,\n",
        "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
        "    # the first dimension to be the timesteps.\n",
        "    model.add(layers.LSTM(128, return_sequences=True))\n",
        "\n",
        "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
        "# of the output sequence, decide which character should be chosen.\n",
        "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k967u9baKY1"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86zAQlLuaKY2",
        "outputId": "bb90f2c4-0b0c-4bf9-e50e-16adcefa7ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 1\n",
            "1407/1407 [==============================] - 22s 10ms/step - loss: 1.7707 - accuracy: 0.3541 - val_loss: 1.5787 - val_accuracy: 0.4076\n",
            "Q 97+43   T 140  ☒ 144 \n",
            "Q 56+12   T 68   ☒ 11  \n",
            "Q 50+819  T 869  ☒ 995 \n",
            "Q 295+34  T 329  ☒ 495 \n",
            "Q 9+924   T 933  ☒ 909 \n",
            "Q 940+45  T 985  ☒ 902 \n",
            "Q 625+14  T 639  ☒ 666 \n",
            "Q 42+973  T 1015 ☒ 901 \n",
            "Q 57+394  T 451  ☒ 499 \n",
            "Q 6+732   T 738  ☒ 744 \n",
            "\n",
            "Iteration 2\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3514 - accuracy: 0.4946 - val_loss: 1.1516 - val_accuracy: 0.5713\n",
            "Q 590+728 T 1318 ☒ 1357\n",
            "Q 8+957   T 965  ☒ 960 \n",
            "Q 440+223 T 663  ☒ 677 \n",
            "Q 58+135  T 193  ☒ 277 \n",
            "Q 779+828 T 1607 ☒ 1547\n",
            "Q 29+557  T 586  ☒ 584 \n",
            "Q 929+86  T 1015 ☒ 1011\n",
            "Q 148+829 T 977  ☒ 904 \n",
            "Q 29+348  T 377  ☒ 471 \n",
            "Q 452+67  T 519  ☒ 514 \n",
            "\n",
            "Iteration 3\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0270 - accuracy: 0.6216 - val_loss: 0.9245 - val_accuracy: 0.6556\n",
            "Q 20+26   T 46   ☒ 43  \n",
            "Q 14+114  T 128  ☒ 111 \n",
            "Q 185+21  T 206  ☑ 206 \n",
            "Q 872+958 T 1830 ☒ 1811\n",
            "Q 47+86   T 133  ☒ 139 \n",
            "Q 46+759  T 805  ☒ 806 \n",
            "Q 94+55   T 149  ☒ 146 \n",
            "Q 92+402  T 494  ☒ 496 \n",
            "Q 317+77  T 394  ☒ 386 \n",
            "Q 932+22  T 954  ☒ 956 \n",
            "\n",
            "Iteration 4\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.8466 - accuracy: 0.6894 - val_loss: 0.7941 - val_accuracy: 0.7046\n",
            "Q 32+564  T 596  ☒ 600 \n",
            "Q 84+140  T 224  ☒ 226 \n",
            "Q 87+748  T 835  ☒ 831 \n",
            "Q 3+80    T 83   ☒ 82  \n",
            "Q 639+49  T 688  ☒ 690 \n",
            "Q 523+831 T 1354 ☒ 1358\n",
            "Q 18+758  T 776  ☒ 780 \n",
            "Q 37+374  T 411  ☒ 416 \n",
            "Q 660+32  T 692  ☒ 690 \n",
            "Q 305+353 T 658  ☒ 650 \n",
            "\n",
            "Iteration 5\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.7393 - accuracy: 0.7306 - val_loss: 0.6924 - val_accuracy: 0.7483\n",
            "Q 654+138 T 792  ☑ 792 \n",
            "Q 57+59   T 116  ☒ 112 \n",
            "Q 33+998  T 1031 ☑ 1031\n",
            "Q 55+67   T 122  ☑ 122 \n",
            "Q 932+22  T 954  ☒ 955 \n",
            "Q 706+92  T 798  ☒ 799 \n",
            "Q 122+37  T 159  ☒ 154 \n",
            "Q 655+38  T 693  ☒ 692 \n",
            "Q 94+901  T 995  ☒ 997 \n",
            "Q 17+0    T 17   ☒ 11  \n",
            "\n",
            "Iteration 6\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.6341 - accuracy: 0.7706 - val_loss: 0.5509 - val_accuracy: 0.7963\n",
            "Q 8+586   T 594  ☒ 595 \n",
            "Q 825+90  T 915  ☒ 916 \n",
            "Q 135+572 T 707  ☑ 707 \n",
            "Q 29+419  T 448  ☒ 445 \n",
            "Q 415+54  T 469  ☑ 469 \n",
            "Q 597+57  T 654  ☒ 655 \n",
            "Q 67+835  T 902  ☑ 902 \n",
            "Q 576+173 T 749  ☒ 750 \n",
            "Q 69+239  T 308  ☒ 306 \n",
            "Q 897+5   T 902  ☒ 901 \n",
            "\n",
            "Iteration 7\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.3888 - accuracy: 0.8692 - val_loss: 0.2959 - val_accuracy: 0.8990\n",
            "Q 3+927   T 930  ☑ 930 \n",
            "Q 488+49  T 537  ☒ 536 \n",
            "Q 84+307  T 391  ☒ 392 \n",
            "Q 883+74  T 957  ☑ 957 \n",
            "Q 539+739 T 1278 ☒ 1277\n",
            "Q 44+544  T 588  ☑ 588 \n",
            "Q 881+387 T 1268 ☒ 1258\n",
            "Q 78+36   T 114  ☑ 114 \n",
            "Q 6+464   T 470  ☑ 470 \n",
            "Q 498+5   T 503  ☒ 502 \n",
            "\n",
            "Iteration 8\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.2126 - accuracy: 0.9444 - val_loss: 0.2139 - val_accuracy: 0.9352\n",
            "Q 87+10   T 97   ☑ 97  \n",
            "Q 272+28  T 300  ☑ 300 \n",
            "Q 11+93   T 104  ☑ 104 \n",
            "Q 7+86    T 93   ☑ 93  \n",
            "Q 171+1   T 172  ☑ 172 \n",
            "Q 111+36  T 147  ☑ 147 \n",
            "Q 111+36  T 147  ☑ 147 \n",
            "Q 714+420 T 1134 ☑ 1134\n",
            "Q 221+18  T 239  ☒ 249 \n",
            "Q 235+24  T 259  ☒ 269 \n",
            "\n",
            "Iteration 9\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.1308 - accuracy: 0.9702 - val_loss: 0.1119 - val_accuracy: 0.9724\n",
            "Q 272+204 T 476  ☒ 376 \n",
            "Q 37+734  T 771  ☑ 771 \n",
            "Q 38+41   T 79   ☑ 79  \n",
            "Q 133+85  T 218  ☑ 218 \n",
            "Q 664+96  T 760  ☑ 760 \n",
            "Q 10+77   T 87   ☑ 87  \n",
            "Q 98+782  T 880  ☒ 870 \n",
            "Q 3+533   T 536  ☑ 536 \n",
            "Q 968+45  T 1013 ☑ 1013\n",
            "Q 6+902   T 908  ☑ 908 \n",
            "\n",
            "Iteration 10\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0927 - accuracy: 0.9785 - val_loss: 0.0757 - val_accuracy: 0.9808\n",
            "Q 725+505 T 1230 ☑ 1230\n",
            "Q 774+352 T 1126 ☑ 1126\n",
            "Q 295+34  T 329  ☑ 329 \n",
            "Q 579+896 T 1475 ☑ 1475\n",
            "Q 32+4    T 36   ☑ 36  \n",
            "Q 89+832  T 921  ☑ 921 \n",
            "Q 466+25  T 491  ☑ 491 \n",
            "Q 687+11  T 698  ☑ 698 \n",
            "Q 23+129  T 152  ☑ 152 \n",
            "Q 683+262 T 945  ☑ 945 \n",
            "\n",
            "Iteration 11\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0697 - accuracy: 0.9836 - val_loss: 0.0518 - val_accuracy: 0.9890\n",
            "Q 29+666  T 695  ☑ 695 \n",
            "Q 8+184   T 192  ☑ 192 \n",
            "Q 151+642 T 793  ☒ 893 \n",
            "Q 841+2   T 843  ☑ 843 \n",
            "Q 311+27  T 338  ☑ 338 \n",
            "Q 54+380  T 434  ☑ 434 \n",
            "Q 303+78  T 381  ☑ 381 \n",
            "Q 307+460 T 767  ☑ 767 \n",
            "Q 884+387 T 1271 ☑ 1271\n",
            "Q 298+91  T 389  ☑ 389 \n",
            "\n",
            "Iteration 12\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0471 - accuracy: 0.9898 - val_loss: 0.0344 - val_accuracy: 0.9928\n",
            "Q 907+503 T 1410 ☑ 1410\n",
            "Q 52+228  T 280  ☑ 280 \n",
            "Q 48+954  T 1002 ☑ 1002\n",
            "Q 48+36   T 84   ☑ 84  \n",
            "Q 958+15  T 973  ☑ 973 \n",
            "Q 625+721 T 1346 ☑ 1346\n",
            "Q 775+91  T 866  ☑ 866 \n",
            "Q 93+179  T 272  ☑ 272 \n",
            "Q 145+77  T 222  ☑ 222 \n",
            "Q 172+0   T 172  ☑ 172 \n",
            "\n",
            "Iteration 13\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0476 - accuracy: 0.9878 - val_loss: 0.0246 - val_accuracy: 0.9959\n",
            "Q 372+365 T 737  ☑ 737 \n",
            "Q 29+508  T 537  ☑ 537 \n",
            "Q 83+169  T 252  ☑ 252 \n",
            "Q 548+79  T 627  ☑ 627 \n",
            "Q 381+573 T 954  ☑ 954 \n",
            "Q 60+289  T 349  ☑ 349 \n",
            "Q 8+586   T 594  ☑ 594 \n",
            "Q 6+464   T 470  ☑ 470 \n",
            "Q 216+833 T 1049 ☑ 1049\n",
            "Q 77+355  T 432  ☑ 432 \n",
            "\n",
            "Iteration 14\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0377 - accuracy: 0.9904 - val_loss: 0.0514 - val_accuracy: 0.9847\n",
            "Q 340+68  T 408  ☑ 408 \n",
            "Q 354+64  T 418  ☑ 418 \n",
            "Q 10+429  T 439  ☑ 439 \n",
            "Q 15+978  T 993  ☑ 993 \n",
            "Q 3+600   T 603  ☑ 603 \n",
            "Q 80+35   T 115  ☑ 115 \n",
            "Q 101+6   T 107  ☑ 107 \n",
            "Q 73+57   T 130  ☑ 130 \n",
            "Q 438+246 T 684  ☑ 684 \n",
            "Q 2+188   T 190  ☑ 190 \n",
            "\n",
            "Iteration 15\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0278 - accuracy: 0.9938 - val_loss: 0.0634 - val_accuracy: 0.9795\n",
            "Q 46+713  T 759  ☑ 759 \n",
            "Q 34+496  T 530  ☑ 530 \n",
            "Q 8+510   T 518  ☑ 518 \n",
            "Q 45+213  T 258  ☑ 258 \n",
            "Q 26+662  T 688  ☑ 688 \n",
            "Q 75+291  T 366  ☒ 365 \n",
            "Q 268+660 T 928  ☑ 928 \n",
            "Q 679+38  T 717  ☑ 717 \n",
            "Q 432+30  T 462  ☑ 462 \n",
            "Q 1+824   T 825  ☑ 825 \n",
            "\n",
            "Iteration 16\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0368 - accuracy: 0.9903 - val_loss: 0.0146 - val_accuracy: 0.9973\n",
            "Q 400+45  T 445  ☑ 445 \n",
            "Q 86+17   T 103  ☑ 103 \n",
            "Q 422+726 T 1148 ☑ 1148\n",
            "Q 93+147  T 240  ☑ 240 \n",
            "Q 901+974 T 1875 ☑ 1875\n",
            "Q 345+11  T 356  ☑ 356 \n",
            "Q 109+726 T 835  ☑ 835 \n",
            "Q 302+357 T 659  ☑ 659 \n",
            "Q 993+990 T 1983 ☑ 1983\n",
            "Q 548+14  T 562  ☑ 562 \n",
            "\n",
            "Iteration 17\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0290 - accuracy: 0.9922 - val_loss: 0.0275 - val_accuracy: 0.9932\n",
            "Q 146+938 T 1084 ☑ 1084\n",
            "Q 14+806  T 820  ☑ 820 \n",
            "Q 3+608   T 611  ☑ 611 \n",
            "Q 818+242 T 1060 ☑ 1060\n",
            "Q 977+78  T 1055 ☑ 1055\n",
            "Q 55+181  T 236  ☑ 236 \n",
            "Q 942+5   T 947  ☑ 947 \n",
            "Q 422+43  T 465  ☑ 465 \n",
            "Q 403+60  T 463  ☑ 463 \n",
            "Q 609+540 T 1149 ☑ 1149\n",
            "\n",
            "Iteration 18\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0286 - accuracy: 0.9924 - val_loss: 0.0110 - val_accuracy: 0.9981\n",
            "Q 518+834 T 1352 ☑ 1352\n",
            "Q 158+29  T 187  ☑ 187 \n",
            "Q 240+3   T 243  ☑ 243 \n",
            "Q 29+557  T 586  ☑ 586 \n",
            "Q 74+637  T 711  ☑ 711 \n",
            "Q 460+6   T 466  ☑ 466 \n",
            "Q 260+1   T 261  ☑ 261 \n",
            "Q 57+275  T 332  ☑ 332 \n",
            "Q 800+79  T 879  ☑ 879 \n",
            "Q 270+35  T 305  ☑ 305 \n",
            "\n",
            "Iteration 19\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0292 - accuracy: 0.9922 - val_loss: 0.0148 - val_accuracy: 0.9973\n",
            "Q 427+33  T 460  ☑ 460 \n",
            "Q 523+87  T 610  ☑ 610 \n",
            "Q 205+77  T 282  ☑ 282 \n",
            "Q 211+181 T 392  ☑ 392 \n",
            "Q 893+907 T 1800 ☑ 1800\n",
            "Q 346+30  T 376  ☑ 376 \n",
            "Q 55+181  T 236  ☑ 236 \n",
            "Q 822+19  T 841  ☑ 841 \n",
            "Q 56+115  T 171  ☑ 171 \n",
            "Q 352+3   T 355  ☑ 355 \n",
            "\n",
            "Iteration 20\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0233 - accuracy: 0.9939 - val_loss: 0.0171 - val_accuracy: 0.9965\n",
            "Q 74+722  T 796  ☑ 796 \n",
            "Q 788+630 T 1418 ☑ 1418\n",
            "Q 64+54   T 118  ☑ 118 \n",
            "Q 582+9   T 591  ☑ 591 \n",
            "Q 50+942  T 992  ☑ 992 \n",
            "Q 517+39  T 556  ☑ 556 \n",
            "Q 653+40  T 693  ☑ 693 \n",
            "Q 42+280  T 322  ☑ 322 \n",
            "Q 710+324 T 1034 ☑ 1034\n",
            "Q 616+132 T 748  ☑ 748 \n",
            "\n",
            "Iteration 21\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 0.0066 - val_accuracy: 0.9987\n",
            "Q 35+880  T 915  ☑ 915 \n",
            "Q 57+275  T 332  ☑ 332 \n",
            "Q 785+83  T 868  ☑ 868 \n",
            "Q 7+974   T 981  ☑ 981 \n",
            "Q 61+320  T 381  ☑ 381 \n",
            "Q 148+829 T 977  ☑ 977 \n",
            "Q 67+424  T 491  ☑ 491 \n",
            "Q 6+143   T 149  ☑ 149 \n",
            "Q 83+169  T 252  ☑ 252 \n",
            "Q 8+375   T 383  ☑ 383 \n",
            "\n",
            "Iteration 22\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0350 - accuracy: 0.9905 - val_loss: 0.0069 - val_accuracy: 0.9991\n",
            "Q 686+403 T 1089 ☑ 1089\n",
            "Q 267+35  T 302  ☑ 302 \n",
            "Q 98+85   T 183  ☑ 183 \n",
            "Q 171+488 T 659  ☑ 659 \n",
            "Q 5+83    T 88   ☑ 88  \n",
            "Q 897+93  T 990  ☑ 990 \n",
            "Q 21+984  T 1005 ☑ 1005\n",
            "Q 15+321  T 336  ☑ 336 \n",
            "Q 54+912  T 966  ☑ 966 \n",
            "Q 541+922 T 1463 ☑ 1463\n",
            "\n",
            "Iteration 23\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0225 - accuracy: 0.9937 - val_loss: 0.0559 - val_accuracy: 0.9802\n",
            "Q 912+9   T 921  ☑ 921 \n",
            "Q 142+215 T 357  ☑ 357 \n",
            "Q 405+14  T 419  ☑ 419 \n",
            "Q 13+35   T 48   ☑ 48  \n",
            "Q 589+95  T 684  ☑ 684 \n",
            "Q 613+47  T 660  ☑ 660 \n",
            "Q 257+2   T 259  ☑ 259 \n",
            "Q 68+86   T 154  ☑ 154 \n",
            "Q 638+66  T 704  ☑ 704 \n",
            "Q 9+987   T 996  ☑ 996 \n",
            "\n",
            "Iteration 24\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0258 - accuracy: 0.9931 - val_loss: 0.0094 - val_accuracy: 0.9981\n",
            "Q 3+80    T 83   ☑ 83  \n",
            "Q 13+42   T 55   ☑ 55  \n",
            "Q 78+334  T 412  ☑ 412 \n",
            "Q 210+914 T 1124 ☑ 1124\n",
            "Q 4+479   T 483  ☑ 483 \n",
            "Q 61+53   T 114  ☑ 114 \n",
            "Q 498+91  T 589  ☑ 589 \n",
            "Q 6+490   T 496  ☑ 496 \n",
            "Q 552+950 T 1502 ☑ 1502\n",
            "Q 377+571 T 948  ☑ 948 \n",
            "\n",
            "Iteration 25\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 0.0275 - val_accuracy: 0.9920\n",
            "Q 543+629 T 1172 ☑ 1172\n",
            "Q 0+440   T 440  ☑ 440 \n",
            "Q 6+753   T 759  ☑ 759 \n",
            "Q 6+917   T 923  ☑ 923 \n",
            "Q 250+739 T 989  ☑ 989 \n",
            "Q 43+845  T 888  ☑ 888 \n",
            "Q 26+37   T 63   ☑ 63  \n",
            "Q 103+67  T 170  ☑ 170 \n",
            "Q 494+2   T 496  ☑ 496 \n",
            "Q 45+618  T 663  ☑ 663 \n",
            "\n",
            "Iteration 26\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0218 - accuracy: 0.9938 - val_loss: 0.0069 - val_accuracy: 0.9988\n",
            "Q 136+1   T 137  ☑ 137 \n",
            "Q 776+465 T 1241 ☑ 1241\n",
            "Q 641+50  T 691  ☑ 691 \n",
            "Q 7+67    T 74   ☑ 74  \n",
            "Q 127+0   T 127  ☑ 127 \n",
            "Q 448+733 T 1181 ☑ 1181\n",
            "Q 438+9   T 447  ☑ 447 \n",
            "Q 575+805 T 1380 ☑ 1380\n",
            "Q 944+157 T 1101 ☑ 1101\n",
            "Q 806+641 T 1447 ☑ 1447\n",
            "\n",
            "Iteration 27\n",
            "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0134 - accuracy: 0.9964 - val_loss: 0.0045 - val_accuracy: 0.9994\n",
            "Q 771+32  T 803  ☑ 803 \n",
            "Q 64+64   T 128  ☑ 128 \n",
            "Q 4+17    T 21   ☑ 21  \n",
            "Q 639+654 T 1293 ☑ 1293\n",
            "Q 9+813   T 822  ☑ 822 \n",
            "Q 417+322 T 739  ☑ 739 \n",
            "Q 20+573  T 593  ☑ 593 \n",
            "Q 42+66   T 108  ☑ 108 \n",
            "Q 738+5   T 743  ☑ 743 \n",
            "Q 781+569 T 1350 ☑ 1350\n",
            "\n",
            "Iteration 28\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
            "Q 16+372  T 388  ☑ 388 \n",
            "Q 136+1   T 137  ☑ 137 \n",
            "Q 59+391  T 450  ☑ 450 \n",
            "Q 356+9   T 365  ☑ 365 \n",
            "Q 52+889  T 941  ☑ 941 \n",
            "Q 99+208  T 307  ☑ 307 \n",
            "Q 628+47  T 675  ☑ 675 \n",
            "Q 91+50   T 141  ☑ 141 \n",
            "Q 770+493 T 1263 ☑ 1263\n",
            "Q 779+828 T 1607 ☑ 1607\n",
            "\n",
            "Iteration 29\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.0328 - accuracy: 0.9908 - val_loss: 0.0077 - val_accuracy: 0.9980\n",
            "Q 26+210  T 236  ☑ 236 \n",
            "Q 237+54  T 291  ☑ 291 \n",
            "Q 804+36  T 840  ☑ 840 \n",
            "Q 703+529 T 1232 ☑ 1232\n",
            "Q 605+36  T 641  ☑ 641 \n",
            "Q 28+776  T 804  ☑ 804 \n",
            "Q 307+460 T 767  ☑ 767 \n",
            "Q 13+836  T 849  ☑ 849 \n",
            "Q 710+75  T 785  ☑ 785 \n",
            "Q 151+63  T 214  ☑ 214 \n"
          ]
        }
      ],
      "source": [
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for epoch in range(1, epochs):\n",
        "    print()\n",
        "    print(\"Iteration\", epoch)\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=1,\n",
        "        validation_data=(x_val, y_val),\n",
        "    )\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
        "        print(\"T\", correct, end=\" \")\n",
        "        if correct == guess:\n",
        "            print(\"☑ \" + guess)\n",
        "        else:\n",
        "            print(\"☒ \" + guess)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTqkyAsWaKY2"
      },
      "source": [
        "You'll get to 99+% validation accuracy after ~30 epochs.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "addition_rnn",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}